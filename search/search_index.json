{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\u26a1\ufe0f What is FastEmbed?","text":"<p>FastEmbed is a lightweight, fast, Python library built for embedding generation. We support popular text models. Please open a Github issue if you want us to add a new model.</p> <p>The default embedding supports \"query\" and \"passage\" prefixes for the input text. The default model is Flag Embedding, which is top of the MTEB leaderboard. Here is an example for Retrieval Embedding Generation and how to use FastEmbed with Qdrant.</p> <ol> <li> <p>Light &amp; Fast</p> <ul> <li>Quantized model weights</li> <li>ONNX Runtime for inference via Optimum</li> </ul> </li> <li> <p>Accuracy/Recall</p> <ul> <li>Better than OpenAI Ada-002</li> <li>Default is Flag Embedding, which is top of the MTEB leaderboard</li> <li>List of supported models - including multilingual models</li> </ul> </li> </ol>"},{"location":"#installation","title":"\ud83d\ude80 Installation","text":"<p>To install the FastEmbed library, pip works:</p> <pre><code>pip install fastembed\n</code></pre>"},{"location":"#usage","title":"\ud83d\udcd6 Usage","text":"<pre><code>from fastembed.embedding import FlagEmbedding as Embedding\n\ndocuments: List[str] = [\n    \"passage: Hello, World!\",\n    \"query: Hello, World!\", # these are two different embedding\n    \"passage: This is an example passage.\",\n    \"fastembed is supported by and maintained by Qdrant.\" # You can leave out the prefix but it's recommended\n]\nembedding_model = Embedding(model_name=\"BAAI/bge-base-en\", max_length=512)\nembeddings: List[np.ndarray] = embedding_model.embed(documents) # If you use\n</code></pre>"},{"location":"#usage-with-qdrant","title":"Usage with Qdrant","text":"<p>Installation with Qdrant Client in Python:</p> <pre><code>pip install qdrant-client[fastembed]\n</code></pre> <p>Might have to use <code>pip install 'qdrant-client[fastembed]'</code> on zsh.</p> <pre><code>from qdrant_client import QdrantClient\n\n# Initialize the client\nclient = QdrantClient(\":memory:\")  # or QdrantClient(path=\"path/to/db\")\n\n# Prepare your documents, metadata, and IDs\ndocs = [\"Qdrant has Langchain integrations\", \"Qdrant also has Llama Index integrations\"]\nmetadata = [\n    {\"source\": \"Langchain-docs\"},\n    {\"source\": \"Linkedin-docs\"},\n]\nids = [42, 2]\n\n# Use the new add method\nclient.add(\n    collection_name=\"demo_collection\",\n    documents=docs,\n    metadata=metadata,\n    ids=ids\n)\n\nsearch_result = client.query(\n    collection_name=\"demo_collection\",\n    query_text=\"This is a query document\"\n)\nprint(search_result)\n</code></pre>"},{"location":"Getting%20Started/","title":"Getting Started","text":"<pre><code>!pip install -Uqq fastembed # Install fastembed\n</code></pre> <pre><code>import numpy as np\nfrom fastembed import TextEmbedding\nfrom typing import List\n\n# Example list of documents\ndocuments: List[str] = [\n    \"This is built to be faster and lighter than other embedding libraries e.g. Transformers, Sentence-Transformers, etc.\",\n    \"fastembed is supported by and maintained by Qdrant.\",\n]\n\n# This will trigger the model download and initialization\nembedding_model = TextEmbedding()\nprint(\"The model BAAI/bge-small-en-v1.5 is ready to use.\")\n\nembeddings_generator = embedding_model.embed(documents)  # reminder this is a generator\nembeddings_list = list(embedding_model.embed(documents))\n# you can also convert the generator to a list, and that to a numpy array\nlen(embeddings_list[0])  # Vector of 384 dimensions\n</code></pre> <pre>\n<code>Fetching 9 files:   0%|          | 0/9 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>The model BAAI/bge-small-en-v1.5 is ready to use.\n</code>\n</pre> <pre>\n<code>384</code>\n</pre> <p>&gt; \ud83d\udca1 Why do we use generators? &gt;  &gt; We use them to save memory mostly. Instead of loading all the vectors into memory, we can load them one by one. This is useful when you have a large dataset and you don't want to load all the vectors at once.</p> <pre><code>embeddings_generator = embedding_model.embed(documents)  # reminder this is a generator\n\nfor doc, vector in zip(documents, embeddings_generator):\n    print(\"Document:\", doc)\n    print(f\"Vector of type: {type(vector)} with shape: {vector.shape}\")\n</code></pre> <pre>\n<code>Document: This is built to be faster and lighter than other embedding libraries e.g. Transformers, Sentence-Transformers, etc.\nVector of type: &lt;class 'numpy.ndarray'&gt; with shape: (384,)\nDocument: fastembed is supported by and maintained by Qdrant.\nVector of type: &lt;class 'numpy.ndarray'&gt; with shape: (384,)\n</code>\n</pre> <pre><code>embeddings_list = np.array(\n    list(embedding_model.embed(documents))\n)  # you can also convert the generator to a list, and that to a numpy array\nembeddings_list.shape\n</code></pre> <pre>\n<code>(2, 384)</code>\n</pre> <p>We're using BAAI/bge-small-en-v1.5 a state of the art Flag Embedding model. The model does better than OpenAI text-embedding-ada-002. We've made it even faster by converting it to ONNX format and quantizing the model for you.</p> <pre><code>multilingual_large_model = TextEmbedding(\"intfloat/multilingual-e5-large\")  # This can take a few minutes to download\n</code></pre> <pre>\n<code>Fetching 8 files:   0%|          | 0/8 [00:00&lt;?, ?it/s]</code>\n</pre> <pre><code>np.array(\n    list(multilingual_large_model.embed([\"Hello, world!\", \"\u4f60\u597d\u4e16\u754c\", \"\u00a1Hola Mundo!\", \"\u0928\u092e\u0938\u094d\u0924\u0947!\"]))\n).shape  # Vector of 1024 dimensions\n</code></pre> <pre>\n<code>(4, 1024)</code>\n</pre> <p>Next: Checkout how to use FastEmbed with Qdrant for similarity search: FastEmbed with Qdrant</p>"},{"location":"Getting%20Started/#getting-started","title":"\ud83d\udeb6\ud83c\udffb\u200d\u2642\ufe0f Getting Started","text":"<p>Here you will learn how to use the fastembed package to embed your data into a vector space. The package is designed to be easy to use and fast. It is built on top of the ONNX standard, which allows for fast inference on a variety of hardware (called Runtimes in ONNX). </p>"},{"location":"Getting%20Started/#quick-start","title":"Quick Start","text":"<p>The fastembed package is designed to be easy to use. We'll be using <code>TextEmbedding</code> class. It takes a list of strings as input and returns an generator of vectors. If you're seeing generators for the first time, don't worry, you can convert it to a list using <code>list()</code>.</p> <p>&gt; \ud83d\udca1 You can learn more about generators from Python Wiki</p>"},{"location":"Getting%20Started/#format-of-the-document-list","title":"Format of the Document List","text":"<ol> <li>List of Strings: Your documents must be in a list, and each document must be a string</li> <li>For Retrieval Tasks with our default: If you're working with queries and passages, you can add special labels to them:</li> <li>Queries: Add \"query:\" at the beginning of each query string</li> <li>Passages: Add \"passage:\" at the beginning of each passage string</li> </ol>"},{"location":"Getting%20Started/#beyond-the-default-model","title":"Beyond the default model","text":"<p>The default model is built for speed and efficiency. If you need a more accurate model, you can use the <code>TextEmbedding</code> class to load any model from our list of available models. You can find the list of available models using <code>TextEmbedding.list_supported_models()</code>.</p>"},{"location":"examples/FastEmbed_vs_HF_Comparison/","title":"FastEmbed vs HF Comparison","text":"<pre><code>import time\nfrom typing import Callable, List, Tuple\n\nimport torch.nn.functional as F\nfrom fastembed.embedding import DefaultEmbedding\nimport matplotlib.pyplot as plt\nfrom torch import Tensor\nfrom transformers import AutoModel, AutoTokenizer\n</code></pre> <pre><code>documents: List[str] = [\n    \"Chandrayaan-3 is India's third lunar mission\",\n    \"It aimed to land a rover on the Moon's surface - joining the US, China and Russia\",\n    \"The mission is a follow-up to Chandrayaan-2, which had partial success\",\n    \"Chandrayaan-3 will be launched by the Indian Space Research Organisation (ISRO)\",\n    \"The estimated cost of the mission is around $35 million\",\n    \"It will carry instruments to study the lunar surface and atmosphere\",\n    \"Chandrayaan-3 landed on the Moon's surface on 23rd August 2023\",\n    \"It consists of a lander named Vikram and a rover named Pragyan similar to Chandrayaan-2. Its propulsion module would act like an orbiter.\",\n    \"The propulsion module carries the lander and rover configuration until the spacecraft is in a 100-kilometre (62 mi) lunar orbit\",\n    \"The mission used GSLV Mk III rocket for its launch\",\n    \"Chandrayaan-3 was launched from the Satish Dhawan Space Centre in Sriharikota\",\n    \"Chandrayaan-3 was launched earlier in the year 2023\",\n]\nlen(documents)\n</code></pre> <pre>\n<code>12</code>\n</pre> <pre><code>class HF:\n    \"\"\"\n    HuggingFace Transformer implementation of FlagEmbedding\n    \"\"\"\n\n    def __init__(self, model_id: str):\n        self.model = AutoModel.from_pretrained(model_id)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n\n    def embed(self, texts: List[str]):\n        encoded_input = self.tokenizer(texts, max_length=512, padding=True, truncation=True, return_tensors=\"pt\")\n        model_output = self.model(**encoded_input)\n        sentence_embeddings = model_output[0][:, 0]\n        sentence_embeddings = F.normalize(sentence_embeddings)\n        return sentence_embeddings\n\n\nhf = HF(model_id=\"BAAI/bge-small-en-v1.5\")\nhf.embed(documents).shape\n</code></pre> <pre>\n<code>torch.Size([12, 384])</code>\n</pre> <pre><code>embedding_model = DefaultEmbedding()\n</code></pre> <pre><code>import types\n\n\ndef calculate_time_stats(embed_func: Callable, documents: list, k: int) -&amp;gt; Tuple[float, float, float]:\n    times = []\n    for _ in range(k):\n        # Timing the embed_func call\n        start_time = time.time()\n        embeddings = embed_func(documents)\n        # Force computation if embed_func returns a generator\n        if isinstance(embeddings, types.GeneratorType):\n            embeddings = list(embeddings)\n\n        end_time = time.time()\n        times.append(end_time - start_time)\n\n    # Returning mean, max, and min time for the call\n    return (sum(times) / k, max(times), min(times))\n\n\nhf_stats = calculate_time_stats(hf.embed, documents, k=2)\nprint(f\"Huggingface Transformers (Average, Max, Min): {hf_stats}\")\nfst_stats = calculate_time_stats(embedding_model.embed, documents, k=2)\nprint(f\"FastEmbed (Average, Max, Min): {fst_stats}\")\n</code></pre> <pre>\n<code>Huggingface Transformers (Average, Max, Min): (0.0635751485824585, 0.06534004211425781, 0.06181025505065918)\nFastEmbed (Average, Max, Min): (0.03929698467254639, 0.039344072341918945, 0.03924989700317383)\n</code>\n</pre> <pre><code>def plot_character_per_second_comparison(\n    hf_stats: Tuple[float, float, float], fst_stats: Tuple[float, float, float], documents: list\n):\n    # Calculating total characters in documents\n    total_characters = sum(len(doc) for doc in documents)\n\n    # Calculating characters per second for each model\n    hf_chars_per_sec = total_characters / hf_stats[0]  # Mean time is at index 0\n    fst_chars_per_sec = total_characters / fst_stats[0]\n\n    # Plotting the bar chart\n    models = [\"HF Embed (Torch)\", \"FastEmbed\"]\n    chars_per_sec = [hf_chars_per_sec, fst_chars_per_sec]\n\n    bars = plt.bar(models, chars_per_sec, color=[\"#1f356c\", \"#dd1f4b\"])\n    plt.ylabel(\"Characters per Second\")\n    plt.title(\"Characters Processed per Second Comparison\")\n\n    # Adding the number at the top of each bar\n    for bar, chars in zip(bars, chars_per_sec):\n        plt.text(\n            bar.get_x() + bar.get_width() / 2,\n            bar.get_height(),\n            f\"{chars:.1f}\",\n            ha=\"center\",\n            va=\"bottom\",\n            color=\"#1f356c\",\n            fontsize=12,\n        )\n\n    plt.show()\n\n\nplot_character_per_second_comparison(hf_stats, fst_stats, documents)\n</code></pre> <pre><code>def calculate_cosine_similarity(embeddings1: Tensor, embeddings2: Tensor) -&amp;gt; float:\n    \"\"\"\n    Calculate cosine similarity between two sets of embeddings\n    \"\"\"\n    return F.cosine_similarity(embeddings1, embeddings2).mean().item()\n\n\ncalculate_cosine_similarity(hf.embed(documents), Tensor(list(embedding_model.embed(documents))))\n</code></pre> <pre>\n<code>/var/folders/b4/grpbcmrd36gc7q5_11whbn540000gn/T/ipykernel_32737/1522845950.py:7: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:248.)\n  calculate_cosine_similarity(hf.embed(documents), Tensor(list(embedding_model.embed(documents))))\n</code>\n</pre> <pre>\n<code>0.9165658950805664</code>\n</pre> <p>This indicates the embeddings are quite close to each with a cosine similarity of 0.99 for BAAI/bge-small-en and 0.92 for BAAI/bge-small-en-v1.5. This gives us confidence that the embeddings are the same and we are not sacrificing accuracy for speed.</p>"},{"location":"examples/FastEmbed_vs_HF_Comparison/#huggingface-vs-fastembed","title":"\ud83e\udd17 Huggingface vs \u26a1 FastEmbed\ufe0f","text":"<p>Comparing the performance of Huggingface's \ud83e\udd17 Transformers and \u26a1 FastEmbed\ufe0f on a simple task on the following machine: Apple M2 Max, 32 GB RAM</p>"},{"location":"examples/FastEmbed_vs_HF_Comparison/#imports","title":"\ud83d\udce6 Imports","text":"<p>Importing the necessary libraries for this comparison.</p>"},{"location":"examples/FastEmbed_vs_HF_Comparison/#data","title":"\ud83d\udcd6 Data","text":"<p>data is a list of strings, each string is a document.</p>"},{"location":"examples/FastEmbed_vs_HF_Comparison/#setting-up-huggingface","title":"Setting up \ud83e\udd17 Huggingface","text":"<p>We'll be using the Huggingface Transformers with PyTorch library to generate embeddings. We'll be using the same model across both libraries for a fair(er?) comparison.</p>"},{"location":"examples/FastEmbed_vs_HF_Comparison/#setting-up-fastembed","title":"Setting up \u26a1\ufe0fFastEmbed","text":"<p>Sorry, don't have a lot to set up here. We'll be using the default model, which is Flag Embedding, same as the Huggingface model.</p>"},{"location":"examples/FastEmbed_vs_HF_Comparison/#comparison","title":"\ud83d\udcca Comparison","text":"<p>We'll be comparing the following metrics: Minimum, Maximum, Mean, across k runs. Let's write a function to do that:</p>"},{"location":"examples/FastEmbed_vs_HF_Comparison/#calculating-stats","title":"\ud83d\ude80 Calculating Stats","text":""},{"location":"examples/FastEmbed_vs_HF_Comparison/#results","title":"\ud83d\udcc8 Results","text":"<p>Let's run the comparison and see the results.</p>"},{"location":"examples/FastEmbed_vs_HF_Comparison/#are-the-embeddings-the-same","title":"Are the Embeddings the same?","text":"<p>This is a very important question. Let's see if the embeddings are the same.</p>"},{"location":"examples/Hindi_Tamil_RAG_with_Navarasa7B/","title":"Hindi Tamil RAG with Navarasa7B","text":"Time: 25 min Level: Beginner Author Nirant Kasliwal <pre><code>!pip install -U fastembed datasets qdrant-client peft transformers accelerate bitsandbytes -qq\n</code></pre> <pre><code>from datasets import load_dataset\nfrom fastembed import TextEmbedding\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.http.models import PointStruct, VectorParams, Distance\nfrom peft import AutoPeftModelForCausalLM\nfrom transformers import AutoTokenizer\nfrom typing import List\nimport numpy as np\n</code></pre> <pre><code>hf_token = \"&lt;your_hf_token_here&gt;\"  # Get your token from https://huggingface.co/settings/token, needed for Gemma weights\n</code></pre> <pre><code>embedding_model = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\nmodel_id = \"Telugu-LLM-Labs/Indic-gemma-7b-finetuned-sft-Navarasa\"\n</code></pre> <pre><code>ds = load_dataset(\"nirantk/chaii-hindi-and-tamil-question-answering\", split=\"train\")\n</code></pre> <pre><code>ds\n</code></pre> <p>This dataset has questions and contexts which have corresponding answers. The answers must be found by the LLM. This is an extractive Question Answering problem.</p> <p>In order to do this, we'll setup an embedding model from FastEmbed. And then add it to Qdrant in memory mode, which is powered by Numpy.</p> <pre><code>embedding_model = TextEmbedding(model_name=embedding_model)\n</code></pre> <p>We'll use the 7B model here, the 2B model isn't great and was suffering from reading comprehension challenges.</p> <pre><code>model = AutoPeftModelForCausalLM.from_pretrained(\n    model_id,\n    load_in_4bit=False,\n    token=hf_token,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n</code></pre> <pre><code>questions, contexts = list(ds[\"question\"]), list(ds[\"context\"])\n</code></pre> <pre><code>context_embeddings: List[np.ndarray] = list(\n    embedding_model.embed(contexts)\n)  # Note the list() call - this is a generator\n</code></pre> <pre><code>len(context_embeddings[0])\n</code></pre> <pre><code>def embed_text(text: str) -&amp;gt; np.array:\n    return list(embedding_model.embed(text))[0]\n</code></pre> <pre><code>context_points = [\n    PointStruct(id=idx, vector=emb, payload={\"text\": text})\n    for idx, (emb, text) in enumerate(zip(context_embeddings, contexts))\n]\n</code></pre> <pre><code>len(context_points[0].vector)\n</code></pre> <pre><code>search_client = QdrantClient(\":memory:\")\n\nsearch_client.create_collection(\n    collection_name=\"hindi_tamil_contexts\",\n    vectors_config=VectorParams(size=len(context_points[0].vector), distance=Distance.COSINE),\n)\nsearch_client.upsert(collection_name=\"hindi_tamil_contexts\", points=context_points)\n</code></pre> <pre><code>idx = 997\n\nquestion = questions[idx]\nprint(question)\nsearch_context = search_client.search(\n    query_vector=embed_text(question), collection_name=\"hindi_tamil_contexts\", limit=2\n)\n</code></pre> <pre><code>search_context_text = search_context[0].payload[\"text\"]\nlen(search_context_text)\n</code></pre> <pre><code>input_prompt = \"\"\"\nAnswer the following question based on the context given after it in the same language as the question:\n### Question:\n{}\n\n### Context:\n{}\n\n### Answer:\n{}\"\"\"\n\ninput_text = input_prompt.format(\n    questions[idx],  # question\n    search_context_text[:2000],  # context\n    \"\",  # output - leave this blank for generation!\n)\n\ninputs = tokenizer([input_text], return_tensors=\"pt\")\n\noutputs = model.generate(**inputs, max_new_tokens=50, use_cache=True)\nresponse = tokenizer.batch_decode(outputs)[0]\n</code></pre> <pre><code>response.split(sep=\"### Answer:\")[-1].strip(\"&lt;eos&gt;\").strip()\n</code></pre> <pre><code>ds[idx][\"answer_text\"]\n</code></pre>"},{"location":"examples/Hindi_Tamil_RAG_with_Navarasa7B/#hindi-and-tamil-question-answer-rag","title":"Hindi and Tamil Question Answer / RAG","text":"<p>In this notebook, we use new Navarasa LLMs from TeluguLLM to create a Hindi and Tamil Question Answering system. Since we're using a 7B model with PEFT, this notebook is run on Google Colab with an A100. If you're working with a smaller machine, I'd encourage to try the 2B model instead.</p>"},{"location":"examples/Hindi_Tamil_RAG_with_Navarasa7B/#setting-up","title":"Setting Up","text":"<p>We'll download the dataset, our LLM model weights and embedding model weights next</p>"},{"location":"examples/Hindi_Tamil_RAG_with_Navarasa7B/#downloading-the-navarasa-llm","title":"Downloading the Navarasa LLM","text":"<p>We'll download the Navarasa LLM from TeluguLLM-Labs. This is a 7B model with PEFT.</p>"},{"location":"examples/Hindi_Tamil_RAG_with_Navarasa7B/#embed-the-context-into-vectors","title":"Embed the Context into Vectors","text":""},{"location":"examples/Hindi_Tamil_RAG_with_Navarasa7B/#insert-into-qdrant","title":"Insert into Qdrant","text":""},{"location":"examples/Hindi_Tamil_RAG_with_Navarasa7B/#selecting-a-question","title":"Selecting a Question","text":"<p>I've randomly selected a question here, with a specific and we then find the answer to it. We have the correct answer for it too -- so we can compare the two when you run the code.</p>"},{"location":"examples/Hindi_Tamil_RAG_with_Navarasa7B/#running-the-model-with-a-question-context","title":"Running the Model with a Question &amp; Context","text":""},{"location":"examples/Retrieval_with_FastEmbed/","title":"Retrieval with FastEmbed","text":"<pre><code># !pip install fastembed --quiet --upgrade\n</code></pre> <p>Importing the necessary libraries:</p> <pre><code>from typing import List\nimport numpy as np\nfrom fastembed import TextEmbedding\n</code></pre> <pre>\n<code>2024-02-07 22:20:57.013 | WARNING  | fastembed.embedding:&lt;module&gt;:7 - DefaultEmbedding, FlagEmbedding, JinaEmbedding are deprecated. Use TextEmbedding instead.\n</code>\n</pre> <pre><code># Example list of documents\ndocuments: List[str] = [\n    \"Maharana Pratap was a Rajput warrior king from Mewar\",\n    \"He fought against the Mughal Empire led by Akbar\",\n    \"The Battle of Haldighati in 1576 was his most famous battle\",\n    \"He refused to submit to Akbar and continued guerrilla warfare\",\n    \"His capital was Chittorgarh, which he lost to the Mughals\",\n    \"He died in 1597 at the age of 57\",\n    \"Maharana Pratap is considered a symbol of Rajput resistance against foreign rule\",\n    \"His legacy is celebrated in Rajasthan through festivals and monuments\",\n    \"He had 11 wives and 17 sons, including Amar Singh I who succeeded him as ruler of Mewar\",\n    \"His life has been depicted in various films, TV shows, and books\",\n]\n# Initialize the DefaultEmbedding class with the desired parameters\nembedding_model = TextEmbedding(model_name=\"BAAI/bge-small-en\", max_length=512)\n\n# We'll use the passage_embed method to get the embeddings for the documents\nembeddings: List[np.ndarray] = list(\n    embedding_model.passage_embed(documents)\n)  # notice that we are casting the generator to a list\n\nprint(embeddings[0].shape, len(embeddings))\n</code></pre> <pre>\n<code>(384,) 10\n</code>\n</pre> <pre><code>query = \"Who was Maharana Pratap?\"\nquery_embedding = list(embedding_model.query_embed(query))[0]\nplain_query_embedding = list(embedding_model.embed(query))[0]\n\n\ndef print_top_k(query_embedding, embeddings, documents, k=5):\n    # use numpy to calculate the cosine similarity between the query and the documents\n    scores = np.dot(embeddings, query_embedding)\n    # sort the scores in descending order\n    sorted_scores = np.argsort(scores)[::-1]\n    # print the top 5\n    for i in range(k):\n        print(f\"Rank {i+1}: {documents[sorted_scores[i]]}\")\n</code></pre> <pre><code>query_embedding[:5], plain_query_embedding[:5]\n</code></pre> <pre>\n<code>(array([-0.04393955,  0.04452892, -0.00760788, -0.03399807,  0.01951348],\n       dtype=float32),\n array([-0.06002192,  0.04322132, -0.00545516, -0.04419701, -0.00542277],\n       dtype=float32))</code>\n</pre> <p>The <code>query_embed</code> is specifically designed for queries, leading to more relevant and context-aware results. The retrieved documents tend to align closely with the query's intent.</p> <p>In contrast, <code>embed</code> is a more general-purpose representation that might not capture the nuances of the query as effectively. The retrieved documents using plain embeddings might be less relevant or ordered differently compared to the results obtained using query embeddings.</p> <p>Conclusion: Using query and passage embeddings leads to more relevant and context-aware results.</p>"},{"location":"examples/Retrieval_with_FastEmbed/#retrieval-with-fastembed","title":"\u2693\ufe0f Retrieval with FastEmbed","text":"<p>This notebook demonstrates how to use FastEmbed to perform vector search and retrieval. It consists of the following sections:</p> <ol> <li>Setup: Installing the necessary packages.</li> <li>Importing Libraries: Importing FastEmbed and other libraries.</li> <li>Data Preparation: Example data and embedding generation.</li> <li>Querying: Defining a function to search documents based on a query.</li> <li>Running Queries: Running example queries.</li> </ol>"},{"location":"examples/Retrieval_with_FastEmbed/#setup","title":"Setup","text":"<p>First, we need to install the dependencies. <code>fastembed</code> to create embeddings and perform retrieval.</p>"},{"location":"examples/Retrieval_with_FastEmbed/#data-preparation","title":"Data Preparation","text":"<p>We initialize the embedding model and generate embeddings for the documents.</p>"},{"location":"examples/Retrieval_with_FastEmbed/#tip-prefer-using-query_embed-for-queries-and-passage_embed-for-documents","title":"\ud83d\udca1 Tip: Prefer using <code>query_embed</code> for queries and <code>passage_embed</code> for documents.","text":""},{"location":"examples/Retrieval_with_FastEmbed/#querying","title":"Querying","text":"<p>We'll define a function to print the top k documents based on a query, and prepare a sample query.</p>"},{"location":"examples/Supported_Models/","title":"Supported Models","text":"<pre><code>%load_ext autoreload\n%autoreload 2\n</code></pre> <pre><code>from fastembed import TextEmbedding\nimport pandas as pd\n\npd.set_option(\"display.max_colwidth\", None)\npd.DataFrame(TextEmbedding.list_supported_models())\n</code></pre> model dim description size_in_GB sources 0 BAAI/bge-base-en 768 Base English model 0.50 {'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-base-en.tar.gz'} 1 BAAI/bge-base-en-v1.5 768 Base English model, v1.5 0.44 {'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-base-en-v1.5.tar.gz', 'hf': 'qdrant/bge-base-en-v1.5-onnx-q'} 2 BAAI/bge-large-en-v1.5-quantized 1024 Large English model, v1.5 1.34 {'hf': 'qdrant/bge-large-en-v1.5-onnx-q'} 3 BAAI/bge-large-en-v1.5 1024 Large English model, v1.5 1.34 {'hf': 'qdrant/bge-large-en-v1.5-onnx'} 4 BAAI/bge-small-en 384 Fast English model 0.20 {'url': 'https://storage.googleapis.com/qdrant-fastembed/BAAI-bge-small-en.tar.gz'} 5 BAAI/bge-small-en-v1.5 384 Fast and Default English model 0.13 {'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-small-en-v1.5.tar.gz', 'hf': 'qdrant/bge-small-en-v1.5-onnx-q'} 6 BAAI/bge-small-zh-v1.5 512 Fast and recommended Chinese model 0.10 {'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-small-zh-v1.5.tar.gz'} 7 sentence-transformers/all-MiniLM-L6-v2 384 Sentence Transformer model, MiniLM-L6-v2 0.09 {'url': 'https://storage.googleapis.com/qdrant-fastembed/sentence-transformers-all-MiniLM-L6-v2.tar.gz', 'hf': 'qdrant/all-MiniLM-L6-v2-onnx'} 8 nomic-ai/nomic-embed-text-v1 768 8192 context length english model 0.54 {'hf': 'nomic-ai/nomic-embed-text-v1'} 9 nomic-ai/nomic-embed-text-v1.5 768 8192 context length english model 0.54 {'hf': 'nomic-ai/nomic-embed-text-v1.5'} 10 thenlper/gte-large 1024 Large general text embeddings model 1.34 {'hf': 'qdrant/gte-large-onnx'} 11 intfloat/multilingual-e5-large 1024 Multilingual model, e5-large. Recommend using this model for non-English languages 2.24 {'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-multilingual-e5-large.tar.gz', 'hf': 'qdrant/multilingual-e5-large-onnx'} 12 sentence-transformers/paraphrase-multilingual-mpnet-base-v2 768 Sentence-transformers model for tasks like clustering or semantic search 1.11 {'hf': 'xenova/paraphrase-multilingual-mpnet-base-v2'} 13 sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 384 Sentence Transformer model, paraphrase-multilingual-MiniLM-L12-v2 0.46 {'hf': 'qdrant/paraphrase-multilingual-MiniLM-L12-v2-onnx-Q'} 14 jinaai/jina-embeddings-v2-base-en 768 English embedding model supporting 8192 sequence length 0.55 {'hf': 'xenova/jina-embeddings-v2-base-en'} 15 jinaai/jina-embeddings-v2-small-en 512 English embedding model supporting 8192 sequence length 0.13 {'hf': 'xenova/jina-embeddings-v2-small-en'}"},{"location":"examples/Usage_With_Qdrant/","title":"Usage With Qdrant","text":"<pre><code>!pip install 'qdrant-client[fastembed]' --quiet --upgrade\n</code></pre> <p>Importing the necessary libraries:</p> <pre><code>from typing import List\nfrom qdrant_client import QdrantClient\n</code></pre> <pre><code># Example list of documents\ndocuments: List[str] = [\n    \"Maharana Pratap was a Rajput warrior king from Mewar\",\n    \"He fought against the Mughal Empire led by Akbar\",\n    \"The Battle of Haldighati in 1576 was his most famous battle\",\n    \"He refused to submit to Akbar and continued guerrilla warfare\",\n    \"His capital was Chittorgarh, which he lost to the Mughals\",\n    \"He died in 1597 at the age of 57\",\n    \"Maharana Pratap is considered a symbol of Rajput resistance against foreign rule\",\n    \"His legacy is celebrated in Rajasthan through festivals and monuments\",\n    \"He had 11 wives and 17 sons, including Amar Singh I who succeeded him as ruler of Mewar\",\n    \"His life has been depicted in various films, TV shows, and books\",\n]\n</code></pre> <p>This tutorial demonstrates how to utilize the QdrantClient to add documents to a collection and query the collection for relevant documents.</p> <pre><code>client = QdrantClient(\":memory:\")\nclient.add(collection_name=\"test_collection\", documents=documents)\n</code></pre> <pre>\n<code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 77.7M/77.7M [00:05&lt;00:00, 14.6MiB/s]\n</code>\n</pre> <pre>\n<code>['4fa8b10c78da4b18ba0830ba8a57367a',\n '2eae04b515ee4e9185a9a0e6be812bba',\n 'c6039f88486f47f1835ae3b069c5823c',\n 'c2c8c51e305144d1917b373125fb4d95',\n '79fd23b9ec0648cdab38d1947c6b933e',\n '036aa200d8c3492b8a438e4f825f5e7f',\n 'c35c77f3ea37460a9a13723fb77b7367',\n '6ebccbca571b40d0ab6e83e5e0f2f562',\n '38048c2ccc1d4962a4f8f1bd89c8357a',\n 'c6b09308360140c7b4f106af3658a31e']</code>\n</pre> <p>These are the ids of the documents we just added. We don't have a use for them in this tutorial, but they can be used to update or delete documents.</p> <pre><code># Prepare your documents, metadata, and IDs\ndocs = [\"Qdrant has Langchain integrations\", \"Qdrant also has Llama Index integrations\"]\nmetadata = [\n    {\"source\": \"Langchain-docs\"},\n    {\"source\": \"Linkedin-docs\"},\n]\nids = [42, 2]\n\n# Use the new add method\nclient.add(collection_name=\"demo_collection\", documents=docs, metadata=metadata, ids=ids)\n</code></pre> <pre>\n<code>[42, 2]</code>\n</pre> <p>Behind the scenes, Qdrant Client uses the FastEmbed library to make a passage embedding and then uses the Qdrant API to upsert the documents with metadata, put together as a Points into the collection.</p> <pre><code>search_result = client.query(collection_name=\"demo_collection\", query_text=\"This is a query document\")\nprint(search_result)\n</code></pre> <pre>\n<code>[QueryResponse(id=42, embedding=None, metadata={'document': 'Qdrant has Langchain integrations', 'source': 'Langchain-docs'}, document='Qdrant has Langchain integrations', score=0.8276550115796268), QueryResponse(id=2, embedding=None, metadata={'document': 'Qdrant also has Llama Index integrations', 'source': 'Linkedin-docs'}, document='Qdrant also has Llama Index integrations', score=0.8265536935180283)]\n</code>\n</pre>"},{"location":"examples/Usage_With_Qdrant/#usage-with-qdrant","title":"Usage With Qdrant","text":"<p>This notebook demonstrates how to use FastEmbed and Qdrant to perform vector search and retrieval. Qdrant is an open-source vector similarity search engine that is used to store, organize, and query collections of high-dimensional vectors. </p> <p>We will use the Qdrant to add a collection of documents to the engine and then query the collection to retrieve the most relevant documents.</p> <p>It consists of the following sections:</p> <ol> <li>Setup: Installing necessary packages, including the Qdrant Client and FastEmbed.</li> <li>Importing Libraries: Importing FastEmbed and other libraries</li> <li>Data Preparation: Example data and embedding generation</li> <li>Querying: Defining a function to search documents based on a query</li> <li>Running Queries: Running example queries</li> </ol>"},{"location":"examples/Usage_With_Qdrant/#setup","title":"Setup","text":"<p>First, we need to install the dependencies. <code>fastembed</code> to create embeddings and perform retrieval, and <code>qdrant-client</code> to interact with the Qdrant database.</p>"},{"location":"examples/Usage_With_Qdrant/#data-preparation","title":"Data Preparation","text":"<p>We initialize the embedding model and generate embeddings for the documents.</p>"},{"location":"examples/Usage_With_Qdrant/#tip-prefer-using-query_embed-for-queries-and-passage_embed-for-documents","title":"\ud83d\udca1 Tip: Prefer using <code>query_embed</code> for queries and <code>passage_embed</code> for documents.","text":""},{"location":"examples/Usage_With_Qdrant/#adding-documents","title":"\u2795 Adding Documents","text":"<p>The <code>add</code> creates a collection if it does not already exist. Now, we can add the documents to the collection:</p>"},{"location":"examples/Usage_With_Qdrant/#running-queries","title":"\ud83d\udcdd Running Queries","text":"<p>We'll define a function to print the top k documents based on a query, and prepare a sample query.</p>"},{"location":"examples/Usage_With_Qdrant/#conclusion","title":"\ud83c\udfac Conclusion","text":"<p>This tutorial demonstrates the basics of working with the QdrantClient to add and query documents. By following this guide, you can easily integrate Qdrant into your projects for vector similarity search and retrieval.</p> <p>Remember to properly handle the closing of the client connection and further customization of the query parameters according to your specific needs.</p> <p>The official Qdrant Python client documentation can be found here for more details on customization and advanced features.</p>"},{"location":"experimental/Binary%20Quantization%20from%20Scratch/","title":"Binary Quantization from Scratch","text":"<pre><code>!pip install matplotlib tqdm pandas numpy --quiet\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n</code></pre> <pre><code>def get_openai_vectors(force_download: bool = False):\n    res = []\n    for i in tqdm(range(26)):\n        if force_download:\n            !wget https://huggingface.co/api/datasets/KShivendu/dbpedia-entities-openai-1M/parquet/KShivendu--dbpedia-entities-openai-1M/train/{i}.parquet\n        df = pd.read_parquet(f\"{i}.parquet\", engine=\"pyarrow\")\n        res.append(np.stack(df.openai))\n        del df\n\n    openai_vectors = np.concatenate(res)\n    del res\n    return openai_vectors\n\n\nopenai_vectors = get_openai_vectors(force_download=False)\nopenai_vectors.shape\n</code></pre> <pre>\n<code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 26/26 [00:10&lt;00:00,  2.45it/s]\n</code>\n</pre> <pre>\n<code>(1000000, 1536)</code>\n</pre> <pre><code>openai_bin = np.zeros_like(openai_vectors, dtype=np.int8)\nopenai_bin[openai_vectors &amp;gt; 0] = 1\n</code></pre> <pre><code>def accuracy(idx, limit: int, oversampling: int):\n    scores = np.dot(openai_vectors, openai_vectors[idx])\n    dot_results = np.argsort(scores)[-limit:][::-1]\n\n    bin_scores = 1536 - np.logical_xor(openai_bin, openai_bin[idx]).sum(axis=1)\n    bin_results = np.argsort(bin_scores)[-(limit * oversampling) :][::-1]\n\n    return len(set(dot_results).intersection(set(bin_results))) / limit\n</code></pre> <pre><code>number_of_samples = 10\nlimits = [10, 100]\nsampling_rate = [1, 2, 3, 5]\nresults = []\n\n\ndef mean_accuracy(number_of_samples, limit, sampling_rate):\n    return np.mean([accuracy(i, limit=limit, oversampling=sampling_rate) for i in range(number_of_samples)])\n\n\nfor i in tqdm(sampling_rate):\n    for j in tqdm(limits):\n        result = {\"sampling_rate\": i, \"limit\": j, \"recall\": mean_accuracy(number_of_samples, j, i)}\n        print(result)\n        results.append(result)\n</code></pre> <pre>\n<code>  0%|          | 0/4 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>{'sampling_rate': 1, 'limit': 10, 'recall': 0.8}\n</code>\n</pre> <pre>\n<code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:33&lt;00:00, 16.98s/it]\n 25%|\u2588\u2588\u258c       | 1/4 [00:33&lt;01:41, 33.96s/it]</code>\n</pre> <pre>\n<code>{'sampling_rate': 1, 'limit': 100, 'recall': 0.708}\n</code>\n</pre> <pre>\n<code></code>\n</pre> <pre>\n<code>{'sampling_rate': 2, 'limit': 10, 'recall': 0.95}\n</code>\n</pre> <pre>\n<code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:32&lt;00:00, 16.38s/it]\n 50%|\u2588\u2588\u2588\u2588\u2588     | 2/4 [01:06&lt;01:06, 33.26s/it]</code>\n</pre> <pre>\n<code>{'sampling_rate': 2, 'limit': 100, 'recall': 0.877}\n</code>\n</pre> <pre>\n<code></code>\n</pre> <pre>\n<code>{'sampling_rate': 3, 'limit': 10, 'recall': 0.96}\n</code>\n</pre> <pre>\n<code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:32&lt;00:00, 16.49s/it]\n 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [01:39&lt;00:33, 33.13s/it]</code>\n</pre> <pre>\n<code>{'sampling_rate': 3, 'limit': 100, 'recall': 0.937}\n</code>\n</pre> <pre>\n<code></code>\n</pre> <pre>\n<code>{'sampling_rate': 5, 'limit': 10, 'recall': 0.9800000000000001}\n</code>\n</pre> <pre>\n<code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:32&lt;00:00, 16.47s/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [02:12&lt;00:00, 33.17s/it]</code>\n</pre> <pre>\n<code>{'sampling_rate': 5, 'limit': 100, 'recall': 0.977}\n</code>\n</pre> <pre>\n<code>\n</code>\n</pre> <pre><code>results = pd.DataFrame(results)\nresults\n</code></pre> sampling_rate limit recall 0 1 10 0.800 1 1 100 0.708 2 2 10 0.950 3 2 100 0.877 4 3 10 0.960 5 3 100 0.937 6 5 10 0.980 7 5 100 0.977 sampling_rate limit accuracy 1 10 0.800 1 100 0.708 2 10 0.950 2 100 0.877 4 10 0.970 4 100 0.956 8 10 0.990 8 100 0.990 16 10 1.000 16 100 0.998"},{"location":"experimental/Binary%20Quantization%20from%20Scratch/#binary-quantization-of-openai-embedding","title":"Binary Quantization of OpenAI Embedding","text":"<p>In the world of large-scale data retrieval and processing, efficiency is crucial. With the exponential growth of data, the ability to retrieve information quickly and accurately can significantly affect system performance. This blog post explores a technique known as binary quantization applied to OpenAI embeddings, demonstrating how it can enhance retrieval latency by 20x or more.</p>"},{"location":"experimental/Binary%20Quantization%20from%20Scratch/#what-are-openai-embeddings","title":"What Are OpenAI Embeddings?","text":"<p>OpenAI embeddings are numerical representations of textual information. They transform text into a vector space where semantically similar texts are mapped close together. This mathematical representation enables computers to understand and process human language more effectively.</p>"},{"location":"experimental/Binary%20Quantization%20from%20Scratch/#binary-quantization","title":"Binary Quantization","text":"<p>Binary quantization is a method which converts continuous numerical values into binary values (0 or 1). It simplifies the data structure, allowing faster computations. Here's a brief overview of the binary quantization process applied to OpenAI embeddings:</p> <ol> <li>Load Embeddings: OpenAI embeddings are loaded from parquet files.</li> <li>Binary Transformation: The continuous valued vectors are converted into binary form. Here, values greater than 0 are set to 1, and others remain 0.</li> <li>Comparison &amp; Retrieval: Binary vectors are used for comparison using logical XOR operations and other efficient algorithms.</li> </ol>"},{"location":"experimental/Binary%20Quantization%20from%20Scratch/#setup-install-dependencies-imports-download-embeddings","title":"Setup: Install Dependencies, Imports &amp; Download Embeddings","text":""},{"location":"experimental/Binary%20Quantization%20from%20Scratch/#code-walkthrough","title":"\ud83d\udc68\ud83c\udffe\u200d\ud83d\udcbb Code Walkthrough","text":"<p>Here's an explanation of the code structure provided:</p> <ol> <li>Loading Data: OpenAI embeddings are loaded from a parquet files (we can load upto 1M embedding) and concatenated into one array.</li> <li>Binary Conversion: A new array with the same shape is initialized with zeros, and the positive values in the original vectors are set to 1.</li> <li>Accuracy Function: The accuracy function compares original vectors with binary vectors for a given index, limit, and oversampling rate. The comparison is done using dot products and logical XOR, sorting the results, and measuring the intersection.</li> <li>Testing: The accuracy is tested for different oversampling rates (1, 2, 4), revealing a correctness of ~0.96 for an oversampling of 4.</li> </ol>"},{"location":"experimental/Binary%20Quantization%20from%20Scratch/#loading-data","title":"\ud83d\udcbf Loading Data","text":""},{"location":"experimental/Binary%20Quantization%20from%20Scratch/#binary-conversion","title":"\u3193 Binary Conversion","text":"<p>Here, we will use 0 as the threshold for the binary conversion. All values greater than 0 will be set to 1, and others will remain 0. This is a simple and effective way to convert continuous values into binary values for OpenAI embeddings.</p>"},{"location":"experimental/Binary%20Quantization%20from%20Scratch/#accuracy-function","title":"\ud83c\udfaf Accuracy Function","text":"<p>We will use the accuracy function to compare the original vectors with the binary vectors for a given index, limit, and oversampling rate. The comparison is done using dot products and logical XOR, sorting the results, and measuring the intersection.</p>"},{"location":"experimental/Binary%20Quantization%20from%20Scratch/#results","title":"\ud83d\udcca Results","text":""},{"location":"experimental/Binary%20Quantization%20with%20Qdrant/","title":"Binary Quantization with Qdrant","text":"<pre><code># !pip install qdrant-client==1.5.1 pandas dataset --quiet --upgrade\n</code></pre> <pre><code>import pandas as pd\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.http import models\n</code></pre> <pre><code>import datasets\n\ndataset = datasets.load_dataset(\"KShivendu/dbpedia-entities-openai-1M\", split=\"train[0:100000]\")\n</code></pre> <pre>\n<code>Resolving data files:   0%|          | 0/26 [00:00&lt;?, ?it/s]</code>\n</pre> <pre><code># !wget https://huggingface.co/datasets/KShivendu/dbpedia-entities-openai-1M/resolve/main/data/train-00000-of-00026-3c7b99d1c7eda36e.parquet -O train.parquet\n</code></pre> <pre><code># df = pd.read_parquet('train.parquet')\n# len(df)\n# dataset = df[:30000]\n# del df\n</code></pre> <pre><code>len(dataset)\n# dataset[0]\n</code></pre> <pre>\n<code>100000</code>\n</pre> <pre><code># client = QdrantClient(\n#     url=\"https://2aaa9439-b209-4ba6-8beb-d0b61dbd9388.us-east-1-0.aws.cloud.qdrant.io:6333\",\n#     api_key=\"FCF8_ADVuSRrtNGeg_rBJvAMJecEDgQhzuXMZGW8F7OzvaC9wYOPeQ\",\n#     prefer_grpc=True\n# )\n\nclient = QdrantClient(\n    url=\"http://localhost:6334\",\n    timeout=600,\n    prefer_grpc=True,\n)\n\ncollection_name = \"binary-quantization\"\nclient.recreate_collection(\n    collection_name=f\"{collection_name}\",\n    vectors_config=models.VectorParams(\n        size=1536,\n        distance=models.Distance.DOT,\n        on_disk=True,\n    ),\n    optimizers_config=models.OptimizersConfigDiff(\n        default_segment_number=5,\n        indexing_threshold=0,\n    ),\n    quantization_config=models.BinaryQuantization(\n        binary=models.BinaryQuantizationConfig(always_ram=True),\n    ),\n)\n</code></pre> <pre>\n<code>True</code>\n</pre> <pre><code># client.upsert(collection_name, points)\nbs = 10000\nfor i in range(0, len(dataset), bs):\n    client.upload_collection(\n        collection_name=collection_name,\n        ids=range(i, i + bs),\n        vectors=dataset[i : i + bs][\"openai\"],\n        payload=[{\"text\": x} for x in dataset[i : i + bs][\"text\"]],\n        parallel=10,\n    )\n</code></pre> <pre><code>client.update_collection(\n    collection_name=f\"{collection_name}\", optimizer_config=models.OptimizersConfigDiff(indexing_threshold=20000)\n)\n</code></pre> <pre>\n<code>True</code>\n</pre> <pre><code>collection_info = client.get_collection(collection_name=f\"{collection_name}\")\ncollection_info.dict()\n</code></pre> <pre>\n<code>/var/folders/b4/grpbcmrd36gc7q5_11whbn540000gn/T/ipykernel_22713/850845717.py:2: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.1.1/migration/\n  collection_info.dict()\n</code>\n</pre> <pre>\n<code>{'status': &lt;CollectionStatus.YELLOW: 'yellow'&gt;,\n 'optimizer_status': &lt;OptimizersStatusOneOf.OK: 'ok'&gt;,\n 'vectors_count': 100000,\n 'indexed_vectors_count': 0,\n 'points_count': 100000,\n 'segments_count': 5,\n 'config': {'params': {'vectors': {'size': 1536,\n    'distance': &lt;Distance.DOT: 'Dot'&gt;,\n    'hnsw_config': None,\n    'quantization_config': None,\n    'on_disk': True},\n   'shard_number': 1,\n   'replication_factor': 1,\n   'write_consistency_factor': 1,\n   'on_disk_payload': True},\n  'hnsw_config': {'m': 16,\n   'ef_construct': 100,\n   'full_scan_threshold': 10000,\n   'max_indexing_threads': 0,\n   'on_disk': False,\n   'payload_m': None},\n  'optimizer_config': {'deleted_threshold': 0.2,\n   'vacuum_min_vector_number': 1000,\n   'default_segment_number': 5,\n   'max_segment_size': None,\n   'memmap_threshold': None,\n   'indexing_threshold': 20000,\n   'flush_interval_sec': 5,\n   'max_optimization_threads': 1},\n  'wal_config': {'wal_capacity_mb': 32, 'wal_segments_ahead': 0},\n  'quantization_config': {'binary': {'always_ram': True}}},\n 'payload_schema': {}}</code>\n</pre> <pre><code>import random\nfrom random import randint\n\nrandom.seed(37)\n\nquery_indices = [randint(0, len(dataset)) for _ in range(100)]\nquery_indices\nquery_dataset = dataset[query_indices]\n</code></pre> <pre><code>## Add Gaussian noise to any vector\nimport numpy as np\n\nnp.random.seed(37)\n\n\ndef add_noise(vector, noise=0.05):\n    return vector + noise * np.random.randn(*vector.shape)\n</code></pre> <pre><code>import time\n\n\ndef correct(results, text):\n    result_texts = [x.payload[\"text\"] for x in results]\n    return text in result_texts\n\n\ndef count_correct(query_dataset, limit=1, oversampling=1, rescore=False):\n    correct_results = 0\n    for qv, text in zip(query_dataset[\"openai\"], query_dataset[\"text\"]):\n        results = client.search(\n            collection_name=collection_name,\n            query_vector=add_noise(np.array(qv)),\n            limit=limit,\n            search_params=models.SearchParams(\n                quantization=models.QuantizationSearchParams(\n                    ignore=False,\n                    rescore=rescore,\n                    oversampling=oversampling,\n                )\n            ),\n        )\n        correct_results += correct(results, text)\n    return correct_results\n\n\nlimit_grid = [1, 3, 5, 10, 20, 50]\n# limit_grid = [1, 3, 5]\noversampling_grid = [1.0, 1.5, 2.0, 3.0, 5.0]\n# oversampling_grid = [1.0, 1.5, 2.0]\nrescore_grid = [False, True]\nresults = []\nfor limit in limit_grid:\n    for oversampling in oversampling_grid:\n        for rescore in rescore_grid:\n            # print(f\"limit={limit}, oversampling={oversampling}, rescore={rescore}\")\n            start = time.time()\n            correct_results = count_correct(query_dataset, limit=limit, oversampling=oversampling, rescore=rescore)\n            end = time.time()\n            results.append(\n                {\n                    \"limit\": limit,\n                    \"oversampling\": oversampling,\n                    \"rescore\": rescore,\n                    \"correct\": correct_results,\n                    \"total queries\": len(query_dataset[\"text\"]),\n                    \"time\": end - start,\n                }\n            )\n\nresults_df = pd.DataFrame(results)\nresults_df\n</code></pre> limit oversampling rescore correct total queries time 0 1 1.0 False 75 4 234.632868 1 1 1.0 True 88 4 0.298480 2 1 1.5 False 75 4 0.225124 3 1 1.5 True 83 4 0.268409 4 1 2.0 False 90 4 0.227887 5 1 2.0 True 90 4 0.368640 6 1 3.0 False 78 4 0.202784 7 1 3.0 True 92 4 0.427767 8 1 5.0 False 72 4 0.196870 9 1 5.0 True 96 4 0.556918 10 3 1.0 False 89 4 0.284389 11 3 1.0 True 92 4 0.469356 12 3 1.5 False 92 4 0.239615 13 3 1.5 True 97 4 0.538105 14 3 2.0 False 91 4 0.231395 15 3 2.0 True 93 4 0.676558 16 3 3.0 False 96 4 0.229494 17 3 3.0 True 95 4 0.894687 18 3 5.0 False 92 4 0.228185 19 3 5.0 True 97 4 1.418588 20 5 1.0 False 93 4 0.251973 21 5 1.0 True 95 4 0.702272 22 5 1.5 False 88 4 0.247825 23 5 1.5 True 97 4 0.832820 24 5 2.0 False 95 4 0.247442 25 5 2.0 True 96 4 0.934421 26 5 3.0 False 92 4 0.234346 27 5 3.0 True 97 4 1.309440 28 5 5.0 False 89 4 0.234266 29 5 5.0 True 97 4 2.010821 30 10 1.0 False 96 4 0.326593 31 10 1.0 True 96 4 1.025037 32 10 1.5 False 89 4 0.304709 33 10 1.5 True 96 4 1.299685 34 10 2.0 False 93 4 0.308194 35 10 2.0 True 98 4 1.680287 36 10 3.0 False 94 4 0.316083 37 10 3.0 True 94 4 2.348110 38 10 5.0 False 97 4 0.334111 39 10 5.0 True 96 4 3.689124 40 20 1.0 False 95 4 0.470449 41 20 1.0 True 98 4 1.808852 42 20 1.5 False 93 4 0.435893 43 20 1.5 True 94 4 2.496987 44 20 2.0 False 96 4 0.473800 45 20 2.0 True 95 4 3.306971 46 20 3.0 False 97 4 0.473543 47 20 3.0 True 100 4 4.618521 48 20 5.0 False 96 4 0.459217 49 20 5.0 True 97 4 7.186773 50 50 1.0 False 95 4 0.891328 51 50 1.0 True 95 4 4.245011 52 50 1.5 False 96 4 0.702362 53 50 1.5 True 98 4 5.981293 54 50 2.0 False 96 4 0.779303 55 50 2.0 True 94 4 7.743587 56 50 3.0 False 94 4 0.816802 57 50 3.0 True 97 4 10.909127 58 50 5.0 False 96 4 0.842580 59 50 5.0 True 99 4 17.818811 <pre><code>df = results_df.copy()\ndf[\"candidates\"] = df[\"oversampling\"] * df[\"limit\"]\ndf[[\"candidates\", \"rescore\", \"time\"]]\n# df.to_csv(\"candidates-rescore-time.csv\", index=False)\n</code></pre> candidates rescore time 0 1.0 False 234.632868 1 1.0 True 0.298480 2 1.5 False 0.225124 3 1.5 True 0.268409 4 2.0 False 0.227887 5 2.0 True 0.368640 6 3.0 False 0.202784 7 3.0 True 0.427767 8 5.0 False 0.196870 9 5.0 True 0.556918 10 3.0 False 0.284389 11 3.0 True 0.469356 12 4.5 False 0.239615 13 4.5 True 0.538105 14 6.0 False 0.231395 15 6.0 True 0.676558 16 9.0 False 0.229494 17 9.0 True 0.894687 18 15.0 False 0.228185 19 15.0 True 1.418588 20 5.0 False 0.251973 21 5.0 True 0.702272 22 7.5 False 0.247825 23 7.5 True 0.832820 24 10.0 False 0.247442 25 10.0 True 0.934421 26 15.0 False 0.234346 27 15.0 True 1.309440 28 25.0 False 0.234266 29 25.0 True 2.010821 30 10.0 False 0.326593 31 10.0 True 1.025037 32 15.0 False 0.304709 33 15.0 True 1.299685 34 20.0 False 0.308194 35 20.0 True 1.680287 36 30.0 False 0.316083 37 30.0 True 2.348110 38 50.0 False 0.334111 39 50.0 True 3.689124 40 20.0 False 0.470449 41 20.0 True 1.808852 42 30.0 False 0.435893 43 30.0 True 2.496987 44 40.0 False 0.473800 45 40.0 True 3.306971 46 60.0 False 0.473543 47 60.0 True 4.618521 48 100.0 False 0.459217 49 100.0 True 7.186773 50 50.0 False 0.891328 51 50.0 True 4.245011 52 75.0 False 0.702362 53 75.0 True 5.981293 54 100.0 False 0.779303 55 100.0 True 7.743587 56 150.0 False 0.816802 57 150.0 True 10.909127 58 250.0 False 0.842580 59 250.0 True 17.818811 <pre><code># client.delete_collection(collection_name=f\"{collection_name}\")\n</code></pre>"},{"location":"experimental/Binary%20Quantization%20with%20Qdrant/#binary-quantization-with-qdrant","title":"Binary Quantization with Qdrant","text":"<p>Binary Quantization is a promising approach to improve retrieval speeds and reduce memory footprint of vector search engines. In this notebook we will show how to use Qdrant to perform binary quantization of vectors and perform fast similarity search on the resulting index.</p>"},{"location":"experimental/Binary%20Quantization%20with%20Qdrant/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Imports</li> <li>Download and Slice Dataset</li> <li>Create Qdrant Collection</li> <li>Indexing</li> <li>Search</li> </ol>"},{"location":"experimental/Binary%20Quantization%20with%20Qdrant/#1-imports","title":"1. Imports","text":""},{"location":"experimental/Binary%20Quantization%20with%20Qdrant/#2-download-and-slice-dataset","title":"2. Download and Slice Dataset","text":"<p>We will be using the dbpedia-entitis-openai-1M dataset from the HuggingFace Datasets library. This contains 1M vectors of 1536 dimensions each. We will be using the first 100K vectors here.</p>"},{"location":"experimental/Binary%20Quantization%20with%20Qdrant/#oversampling-vs-recall","title":"Oversampling vs Recall","text":""},{"location":"experimental/Binary%20Quantization%20with%20Qdrant/#preparing-a-query-dataset","title":"Preparing a query dataset","text":"<p>For the purpose of this illustration, we'll take a few vectors which we know are already in the index and query them. We should get the same vectors back as results from the Qdrant index. </p>"}]}